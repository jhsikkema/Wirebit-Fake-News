import os, os.path
import re
import time
import glob
import dateutil.parser
import codecs
import json
import threading
import hashlib
from collections import deque
from IPFS.ipfs import IPFSGateway
from datetime import datetime, timedelta

from Util.Const import Const
from Util.Config import Config
from Util.Log import Log
import argparse

from Database.Article import Article
from Database.Request import Request

from Database.Database import Database

class Analyzer(threading.Thread):
	def __init__(self, config):
		self.m_config		      = config
		self.m_run		      = True
		self.m_ipfs_gateway	      = IPFSGateway(self.m_config)
		self.m_settings		      = BlockSettings.get()
		self.m_queue	              = deque()

				
	def run(self):
		while self.m_run:
                        queue = Request.getQueue()
                        for item in queue:
                                pass
                        time.sleep(1)


	def stop(self):
		self.m_run = False

	def validate(self, article_hash):
		articles = PublishedArticle.getArticleByHASH(article_hash)
		if len(articles) == 0:
			return ErrorCodes.VALIDATE_ARTICLE_NOT_RECOGNIZED
		article		   = articles[0]
		msg		   = ErrorCodes.VALIDATE_ARTICLE_SUCCESS
		block		   = article.toJSON()
		block['ipfs_hash'] = article.article
		block['id']	   = article.article
		block['url']	   = self.m_config[Const.STORAGE_BLOCK_EXPLORER_URL].format(**block)
		block['ipfs_url_local'] = "http://localhost:5001/api/v0/cat?arg={0:s}".format(str(article_hash))
		block['ipfs_url_global'] = "https://ipfs.io/ipfs/{0:s}".format(str(article_hash))
		msg['message'] = block
		return msg

	def downloadIndex(self, date):
		self.m_ipfs_gateway.retrieveDocument('/bywire/data/index{0:s}'.format(date), lambda x: self.parseIndex(key, x))
	
	def certify(self, article_id, article_hash):
		Log.info("BlockExplorer.certify", article_id, article_hash)
		if (not(article_id) and not(article_hash)):
			return ErrorCodes.CERTIFY_ARTICLE_NOT_SPECIFIED
		if (not(article_hash)):
			articles = Article.getArticleByID(article_id)
			article_hash = articles[0].ipfs_hash
		base_articles = Article.getArticleByHASH(article_hash)
		Log.info("Certify", base_articles)
		if not(base_articles):
			return ErrorCodes.CERTIFY_ARTICLE_NOT_FOUND
		base_article = base_articles[0]
		articles = []
		revisions = {}
		for article in base_articles:
			if article.id in revisions:
				continue
			revisions[article.id] = []
			new_articles = Article.getArticleByID(article.id)
			for new_article in new_articles:
				revision = new_article.revision
				if (revision in revisions[article.id]):
					continue
				revisions[article.id].append(revision)
				articles.append(new_article)
		if not(articles):
			return ErrorCodes.CERTIFY_ARTICLE_NOT_FOUND
		msg = ErrorCodes.CERTIFY_ARTICLE_SUCCESS
		msg['article'] = {}
		msg['article']['revision']  = base_article.revision
		msg['article']['content']   = base_article.content
		msg['article']['title']	    = base_article.title
		msg['article']['author']    = base_article.author
		msg['article']['content_hash'] = base_article.content_hash
		msg['article']['id']	    = base_article.id
		msg['article']['ipfs_hash'] = base_article.ipfs_hash
		msg['article']['timestamp'] = base_article.timestamp.strftime("%Y-%m-%d %H:%M:%S.%f")
		msg['article']['ipfs_url_local'] = "http://localhost:5001/api/v0/cat?arg={0:s}".format(article_hash)
		msg['article']['ipfs_url_global'] = "https://ipfs.io/ipfs/{0:s}".format(article_hash)
		
		msg['revisions'] = []
		for article in articles:
			block = article.toJSON()
			block['ipfs_hash']     = article.ipfs_hash
			block['ipfs_url_local'] = "http://localhost:5001/api/v0/cat?arg={0:s}".format(article_hash)
			block['ipfs_url_global'] = "https://ipfs.io/ipfs/{0:s}".format(article_hash)
			msg['revisions'].append(block)
			
		return msg



if __name__=='__main__':
	parser = argparse.ArgumentParser(description="Run stand alone block explorer")
	parser.add_argument("--load-historic",	     help="Load historic data from file", action="store_true")
	parser.add_argument("--export-historic",     help="Export historic data from file", default="")
	parser.add_argument("--reload-transactions", help="Force loading older transactions", action="store_true")
	args = parser.parse_args()

	os.environ['FLASK_ENV'] = 'development'

	with Config(Const.CONFIG_PATH) as config:
		log = Log(config)
                analyzer = Analyzer(config)
		analyzer.start()
		analyzer.join()
		analyzer.stop()
				


